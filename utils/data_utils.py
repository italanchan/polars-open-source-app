import os
import re
import polars as pl

DATA_SCHEMA = [
    "hvfhs_license_num",
    "dispatching_base_num",
    "originating_base_num",
    "request_datetime",
    "on_scene_datetime",
    "pickup_datetime",
    "dropoff_datetime",
    "PULocationID",
    "DOLocationID",
    "trip_miles",
    "trip_time",
    "base_passenger_fare",
    "tolls",
    "bcf",
    "sales_tax",
    "congestion_surcharge",
    "airport_fee",
    "tips",
    "driver_pay",
    "shared_request_flag",
    "shared_match_flag",
    "access_a_ride_flag",
    "wav_request_flag",
    "wav_match_flag",
]


def get_data_source():
    """
    This function will be called once when app is spun up to create a
    global variable datasource for all data_util functions.

    Logic flow:
        If "data/fhvhv_data.arrow" exist it will return that as source.

        Else if "data/fhvhv_data.arrow" doesn't exist  build a pl.DataFrame
        from all filres in data/ formatted as fhvvhv_tripdata_YYYY_MM_DD.parquet
        with side effect of writing this data source to "data/fhvhv_data.arrow"
        for later calls.

        If no data source found build an empty dataframe with correct schema.

    Returns
    -------
    pl.LazyFrame
    """
    try:
        print("Creating DATA_SOURCE from arrow file")
        data_source = pl.scan_ipc("data/fhvhv_data.arrow")
        return data_source
    except FileNotFoundError:
        print(
            """
            No fhvhv_data.arrow file found, Looking for fhvvhv_tripdata_YYYY_MM_DD.parquet formated files in data/ directory
            """
        )
        try:
            all_files = os.listdir("data/")
        except FileNotFoundError:
            print(
                """
                data/ directory is empty or does not exist. Download the latest "High Volume For-Hire Vehicle Trip Records" 
                parquet file from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page and add them to data/ folder
                """
            )
            data_source = pl.LazyFrame(schema=DATA_SCHEMA)
            return data_source

        df_list = []
        schema = None

        for file_name in all_files:
            if re.search("fhvhv\_tripdata\_\d{4}\-\d{2}.parquet", file_name):
                if not schema:
                    df = pl.read_parquet(f"data/{file_name}")
                    schema = df.schema
                else:
                    df = pl.read_parquet(f"data/{file_name}")
                    df = df.with_columns(
                        [pl.col(col).cast(schema[col]) for col in schema]
                    )

                df_list.append(df)

        if not df_list:
            print(
                """
                Could not find any parquet files in correct format, download the latest "High Volume For-Hire Vehicle Trip Records" 
                parquet file from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page and add them to data/ folder\n
                """
            )
            data_source = pl.LazyFrame(schema=DATA_SCHEMA)
            return data_source
        else:
            print(
                """
                Creating DATA_SOURCE from parquet files. This may take a few seconds...\n"""
            )
            all_data = pl.concat(df.select(df_list[0].columns) for df in df_list)
            all_data.write_ipc("data/fhvhv_data.arrow")
            print(
                """
                DATA_SOURCE successfully created and save to data/ directory as data/fhvhv_data.arrow. 
                App will read from this file in later runs.\n
                """
            )
            return all_data.lazy()


DATA_SOURCE = get_data_source()
column_names = DATA_SOURCE.columns


def scan_ldf(
    filter_model=None,
    columns=None,
    sort_model=None,
):
    """
    This function start with DATA_SOURCE and add select and filter queries on top
    to generate a LazyFrame

    Parameters
    ----------
    filter_model : dict of {str : str}
            A filter model generated by dash-ag-grid component filters
    columns : list of str
            A list of columns to select

    Returns
    -------
    pl.LazyFrame
    """

    ldf = DATA_SOURCE
    ldf = ldf.with_columns(pl.col("request_datetime").cast(pl.Date).alias("event_date"))
    if columns:
        ldf = ldf.select(columns)
    if filter_model:
        expression_list = make_filter_expr_list(filter_model)
        if expression_list:
            filter_query = None
            for expr in expression_list:
                if filter_query is None:
                    filter_query = expr
                else:
                    filter_query &= expr
            ldf = ldf.filter(filter_query)
    return ldf


def make_filter_expr_list(filter_model):
    """
    Genearte all polar filter expressions from filter model

    Parameters
    ----------
    filter_model : dict of {str : str}
            A filter model generated by dash-ag-grid component filters

    Returns
    -------
    list of pl.Expr
    """
    expression_list = []
    for col_name in filter_model:
        if "operator" in filter_model[col_name]:
            if filter_model[col_name]["operator"] == "AND":
                expr1 = parse_column_filter(
                    filter_model[col_name]["condition1"], col_name
                )
                expr2 = parse_column_filter(
                    filter_model[col_name]["condition2"], col_name
                )
                expr = expr1 & expr2
            else:
                expr1 = parse_column_filter(
                    filter_model[col_name]["condition1"], col_name
                )
                expr2 = parse_column_filter(
                    filter_model[col_name]["condition2"], col_name
                )
                expr = expr | expr2
        else:
            expr = parse_column_filter(filter_model[col_name], col_name)
        expression_list.append(expr)

    return expression_list


def get_filter_values(col_name):
    """
    Get a list of unique values in a column
    This is used to create categorical filters in dash-ag-grid
    Parameters
    ----------
    col_name : str
            A name of a column in DATA_SOURCE

    Returns
    -------
    list
    """
    return (
        DATA_SOURCE.select(pl.col(col_name))
        .collect()
        .unique()
        .get_columns()[0]
        .to_list()
    )


def parse_column_filter(filter_obj, col_name):
    """
    Build a polars filter expression for a column based on the corrosponding filter item

    Parameters
    ----------
    col_name : str
            A name of a column in DATA_SOURCE

    Returns
    -------
    pl.Expr
    """
    if filter_obj["filterType"] == "set":
        expr = None
        for val in filter_obj["values"]:
            expr |= pl.col(col_name).cast(pl.Utf8).cast(pl.Categorical) == val
    else:
        if filter_obj["filterType"] == "date":
            crit1 = filter_obj["dateFrom"]

            if "dateTo" in filter_obj:
                crit2 = filter_obj["dateTo"]

        else:
            if "filter" in filter_obj:
                crit1 = filter_obj["filter"]
            if "filterTo" in filter_obj:
                crit2 = filter_obj["filterTo"]

        if filter_obj["type"] == "contains":
            lower = (crit1).lower()
            expr = pl.col(col_name).str.to_lowercase().str.contains(lower)

        elif filter_obj["type"] == "notContains":
            lower = (crit1).lower()
            expr = ~pl.col(col_name).str.to_lowercase().str.contains(lower)
        elif filter_obj["type"] == "startsWith":
            lower = (crit1).lower()
            expr = pl.col(col_name).str.starts_with(lower)

        elif filter_obj["type"] == "notStartsWith":
            lower = (crit1).lower()
            expr = ~pl.col(col_name).str.starts_with(lower)

        elif filter_obj["type"] == "endsWith":
            lower = (crit1).lower()
            expr = pl.col(col_name).str.ends_with(lower)

        elif filter_obj["type"] == "notEndsWith":
            lower = (crit1).lower()
            expr = ~pl.col(col_name).str.ends_with(lower)

        elif filter_obj["type"] == "blank":
            expr = pl.col(col_name).is_null()

        elif filter_obj["type"] == "notBlank":
            expr = ~pl.col(col_name).is_null()

        elif filter_obj["type"] == "equals":
            expr = pl.col(col_name) == crit1

        elif filter_obj["type"] == "notEqual":
            expr = pl.col(col_name) != crit1

        elif filter_obj["type"] == "lessThan":
            expr = pl.col(col_name) < crit1

        elif filter_obj["type"] == "lessThanOrEqual":
            expr = pl.col(col_name) <= crit1

        elif filter_obj["type"] == "greaterThan":
            expr = pl.col(col_name) > crit1

        elif filter_obj["type"] == "greaterThanOrEqual":
            expr = pl.col(col_name) >= crit1

        elif filter_obj["type"] == "inRange":
            if filter_obj["filterType"] == "date":
                expr = (pl.col(col_name) >= crit1) & (pl.col(col_name) <= crit2)
            else:
                expr = (pl.col(col_name) >= crit1) & (pl.col(col_name) <= crit2)
        else:
            None

    return expr


def aggregate_on_trip_distance_time(ldf):
    """
    Aggregate on distance and time data in High Volume For-Hire Vehicle Trip Records.

    Parameters
    ----------
    ldf : pl.LazyFrame

    Returns
    -------
    pl.DataFrame
    """
    results = (
        ldf.with_columns(
            [
                pl.col("trip_miles").round(1).alias("rounded_miles"),
                ((pl.col("trip_time") / 100).round(0) * 100).alias("rounded_time"),
            ]
        )
        .collect()
        .groupby(["rounded_miles", "rounded_time"])
        .count()
    )
    return results


def aggregate_on_pay_tip(ldf):
    """
    Aggregate on driver pay and tips in High Volume For-Hire Vehicle Trip Records.

    Parameters
    ----------
    ldf : pl.LazyFrame

    Returns
    -------
    pl.DataFrame
    """
    results = (
        ldf.with_columns(
            [
                pl.col("driver_pay").round(0).alias("rounded_driver_pay"),
                pl.col("tips").round(0).alias("rounded_tips"),
            ]
        )
        .collect()
        .groupby(["rounded_driver_pay", "rounded_tips"])
        .count()
    )
    return results
